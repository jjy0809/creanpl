import urllib.parse
import urllib.request
import json
import time



q_list = [
    "윤석열", "이재명", "국민의 힘", "민주당", "여가부", "김건희", "대통령", "문재인", "박근혜", "이명박", "종부세", "노조", "파업", "경제", "금리", "탈세", "마약", "사기", "검거", "논란", "특검", "검찰", "스캔들", "삼성", "이재용", "일본", "방사능", "원전", "장애인", "의사", "부동산", "오염수", "중국", "미국", "바이든", "트럼프", "사고", "지진", "징역", "집행유예", "선거", "경상", "경기", "전라", "충청", "강원", "제주", "독도", "시청역", "민희진", "날씨", "정치", "코로나19", "백신", "방역", "정부", "국회", "정책", "소상공인", "중소", "ESG", "환경", "기후변화", "전기차", "반도체", "AI", "LG", "현대자동차", "카카오", "네이버", "벤처", "교육", "청년", "고용", "복지", "연금", "사회복지", "병원", "범죄", "경찰", "군대", "국방", "외교", "북한", "러시아", "유럽", "국제", "통일", "남북관계", "스포츠", "올림픽", "축구", "야구", "연예","여행", "관광", "교통", "자동차", "항공", "철도", "물류", "택배", "소비", "총선", "도발", "백신", "경제 위기", "청년 실업", "대규모 화재", "교통 사고", "사이버 공격", "환경 오염", "군사 훈련", "남북 회담", "대북 제재", "방역", "사회적 거리두기", "의료진 파업", "대선", "법원", "연예인", "협약", "문제", "무역 협상", "사고", "대란", "파업", "한전", "전기차 보급", "탄소 배출", "재생 에너지", "금리 인상", "주식시장 변동", "가상화폐", "데이터 유출", "개인정보", "방송사고", "문화재", "학교 폭력", "학생", "청소년", "노인", "복지 예산", "의료 보험", "전염병", "접종", "안전", "재난", "이민", "다문화", "여성", "성평등", "노동", "임금 인상", "근로", "비정규직", "노사",  "최저임금", "대구", "부산", "대전", "서울", "인천", "광주", "울산", "김정은", "오물 풍선", "M&A", "비트코인", "이더리움", "고속도로", "흡연", "역주행", "코스피", "나스닥", "주식", "코인", "채권", "보험", "보이스피싱", "배임", "횡령", "물가", "게임", "인터넷", "일론머스크", "빌 게이츠", "애플", "저커버그", "팀 쿡", "마이크로소프트", "엔비디아", "인텔", "동물", "학대", "검수완박", "개혁신당", "이준석", "비대위", "법정", "잼버리", "코로나", "전쟁", "이스라엘", "중동", "기름", "휘발유", "필러버스터", "방송", "비리", "뇌물", "해킹", "바이러스", "푸바오", "사제기", "버닝썬", "최순실", "n번방", "텔레그램", "유출", "모의고사", "학력평가", "수능", "세월호", "이태원", "음식", "바가지", "축제", "비", "눈", "폭설", "애플", "마이크로소프트", "개인", "국회", "축제", "학교", "메가스터디", "수능", "모의고사", "학력평가", "시험", "아프가니스탄", "시리아", "내전", "이슬람", "중동", "러시아", "푸틴", "트럼프", "바이든", "헤리스", "오바마", "네이버", "다음", "여성", "남성", "군", "비리", "의혹", "폭로", "폭행", "구속", "수사", "사건", "특종", "전쟁", "필레스타인", "이스라엘", "폭격", "암살", "지진", "태풍", "바이러스", "중국", "반도체", "주식", "메도", "선물", "코인", "가상", "코로나", "메르스", "독감", "병", "연구", "의협", "축구", "손흥민", "클리스만", "국가대표", "올림픽", "아시안게임", "월드컵", "이강인", "교육감", "교육청", "여가부", "전라도", "경상도", "경기도", "평화누리", "제주도", "강원도", "북한", "김정은", "대북", "오물 풍선", "혐의", "검찰", "특검", "경찰", "수사", "대통령", "비서실", "회장", "사장", "기업", "세금", "혈세", "충돌", "고속도로", "복권", "도박", "프랑스", "영국", "대만"
]

# 검색어 목록 중복 제거
q_list = list(set(q_list))

# 링크 파일 경로
link_file_path = r"C:\Users\happy\Desktop\학교\고등학교\2학년\양방향 장단기 메모리 신경망을 이용한 인터넷 혐오표현 검열 확장 프로그램\link.txt"

# 네이버 API 키 입력
client_id = "JJl7umFNxamlNNmfzdXE"
client_secret = "KZXHGHiwnJ"

def get_url(encText, end, display):
    naver_urls = []

    # API 요청
    for start in range(end):
        url = "https://openapi.naver.com/v1/search/news?query=" + encText + "&start=" + str(start * display + 1) + "&display=" + str(display) + "&sort=sim"
        print(url)
        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", client_id)
        request.add_header("X-Naver-Client-Secret", client_secret)
        response = urllib.request.urlopen(request)
        rescode = response.getcode()
        if rescode == 200:
            response_body = response.read()
            data = json.loads(response_body.decode('utf-8'))['items']
            for row in data:
                if 'news.naver' in row['link']:
                    naver_urls.append(row['link'])
            time.sleep(0.1)
        else:
            print("Error Code:" + str(rescode))

    with open(link_file_path, "a", encoding='utf-8') as f:
        for i in naver_urls:
            urli = i[:39] + "comment/" + i[39:]
            if urli not in link_lst:
                f.write(urli + '\n')
                print(urli)
                link_lst.append(urli)

# 링크 목록 읽기
with open(link_file_path, "r", encoding='utf-8') as f:
    link_lst = f.readlines()

# 검색을 끝낼 페이지 입력
end = 5

# 한 번에 가져올 페이지 입력
display = 15

# 검색어 입력
for k in q_list:
    encText = urllib.parse.quote(k)
    print(k)
    get_url(encText, end, display)